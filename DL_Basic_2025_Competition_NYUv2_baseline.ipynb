{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kotaro015/dl_lecture_competition_pub/blob/main/DL_Basic_2025_Competition_NYUv2_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTJ_SWhK1MBp"
      },
      "source": [
        "# Deep Learning 基礎講座　最終課題: NYUv2 セマンティックセグメンテーション\n",
        "\n",
        "## 概要\n",
        "RGB画像から、画像内の各ピクセルがどのクラスに属するかを予測するセマンティックセグメンテーションタスク.\n",
        "\n",
        "### データセット\n",
        "- データセット: NYUv2 dataset\n",
        "- 訓練データ: 795枚\n",
        "- テストデータ: 654枚\n",
        "- 入力: RGB画像 + 深度マップ（元画像サイズは可変）\n",
        "- 出力: 13クラスのセグメンテーションマップ\n",
        "- 評価指標: Mean IoU (Intersection over Union)\n",
        "\n",
        "### データセットの詳細（[NYU Depth Dataset V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)）\n",
        "- 画像は屋内シーンを撮影したもので、家具や壁、床などの物体が含まれています.\n",
        "- 各画像に対して13クラスのセグメンテーションラベルが提供されます.\n",
        "- データは以下のディレクトリ構造で提供:\n",
        "```\n",
        "data/NYUv2/\n",
        "├─train/\n",
        "│  ├─image/      # RGB画像\n",
        "│  │    000000.png\n",
        "│  │    ...\n",
        "│  │\n",
        "│  ├─depth/      # 深度マップ\n",
        "│  │    000000.png\n",
        "│  │    ...\n",
        "│  │\n",
        "│  └─label/      # 13クラスセグメンテーション（教師ラベル）\n",
        "│       000000.png\n",
        "│       ...\n",
        "└─test/\n",
        "   ├─image/      # RGB画像\n",
        "   │    000000.png\n",
        "   │    ...\n",
        "   │  ├─depth/   # 深度マップ\n",
        "   │    000000.png\n",
        "   │    ...\n",
        "```\n",
        "\n",
        "### タスクの詳細\n",
        "- 入力のRGB画像と深度マップから、各ピクセルが13クラスのどれに属するかを予測するタスクです.\n",
        "- 評価はMean IoUを使用します．\n",
        "  - 各クラスごとにIoUを計算し、その平均を取ります.\n",
        "  - IoUは以下の式で計算:\n",
        "  $$IoU = \\frac{TP}{TP + FP + FN}$$\n",
        "    - TP: True Positive（正しく予測されたピクセル数）\n",
        "    - FP: False Positive（誤って予測されたピクセル数）\n",
        "    - FN: False Negative（見逃したピクセル数）\n",
        "\n",
        "### 前処理\n",
        "- 入力画像は512×512にリサイズされます.\n",
        "- ピクセル値は0-1に正規化されます.\n",
        "- セグメンテーションラベルは0-12の整数値（13クラス）です．\n",
        "  - 255はignore index（評価から除外）\n",
        "\n",
        "### 提出形式\n",
        "- テスト画像（RGB + Depth）の各ピクセルに対してクラス（0~12）を予測したものをnumpy配列として保存されます.\n",
        "- ファイル名: `submission.npy`\n",
        "- 配列の形状: [テストデータ数, 高さ, 幅]\n",
        "- 各ピクセルの値: 0-12の整数（予測クラス）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ValDFnY5hc6"
      },
      "source": [
        "## 考えられる工夫の例\n",
        "- 事前学習モデルの fine-tuning\n",
        "    - ImageNetなどで事前学習されたモデルを本データセットでfine-tuningすることで性能向上が見込めます.\n",
        "- 損失関数の再設計\n",
        "    - クラスごとの出現頻度に応じて損失を補正するように損失関数を設計すると、クラス分布の不均衡に対してロバストな学習ができます.\n",
        "- 画像の前処理\n",
        "    - RandomResizedCrop / Flip / ColorJitter 等のデータ拡張を追加することで，汎化性能の向上が見込めます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r3PFWN_8bkJ"
      },
      "source": [
        "## 修了要件を満たす条件\n",
        "- ベースラインでは，omnicampus 上での性能評価において， 38.2% となります．したがって，ベースラインである 38.2% を超えた提出のみ，修了要件として認めます．\n",
        "- ベースラインから改善を加えることで， 50%以上に性能向上することを運営で確認しています．こちらを 1つの指標として取り組んでみてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k52rQxqy8fvT"
      },
      "source": [
        "## 注意点\n",
        "- 学習するモデルについて制限はありませんが，必ず訓練データで学習したモデルで予測してください．\n",
        "    - 事前学習済みモデルを利用して，訓練データを fine-tuning しても構いません．\n",
        "    - 埋め込み抽出モデルなど，モデルの一部を訓練しないケースは構いません．\n",
        "    - 学習を一切せずに，ChatGPT などの基盤モデルを利用することは禁止とします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qGzRC_T8iyS"
      },
      "source": [
        "### データの準備\n",
        "データをダウンロードした際に，google drive したため，利用するために google drive をマウントする必要があります．また， drive 上で展開することができないため，/content ディレクトリ下にコピーし \"data.zip\" を展開します．  \n",
        "google drive 上に \"data.zip\" が配置されていない場合は実行できません．google drive 上に \"data.zip\" (**831MB**) を配置することが可能であれば，\"data_download.ipynb\" を先に実行してください．難しい場合は，omnicampus 演習環境を利用してください．．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPPssX_4XXfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2591cee2-8b13-43e0-f40c-1f7ddb2e7b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# omnicampus 上では 4 セル目まで実行不要\n",
        "# ドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Fl3d2HTRg9"
      },
      "outputs": [],
      "source": [
        "# データダウンロード用の notebook にてgoogle drive への保存後，\n",
        "# 反映に時間がかかる可能性がありますので，google drive のマウント後，\n",
        "# data.zip がディレクトリ内にあることを確認してから実行してください．\n",
        "# data.zip を /content 下にコピーする\n",
        "!cp \"/content/drive/MyDrive/DLBasics2025_colab/最終課題/NYUv2/data.zip\" \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8tO1CHiTSOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ee0d3e-4f78-4f9e-e199-100fc98dcbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  data.zip  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "# カレントディレクトリ下のファイル群を確認\n",
        "# data.zip が表示されれば問題ないです\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zCM4zSbTUbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a4bd80-d73d-4b63-d4b5-dc5e04d2e239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/test/depth/000833.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: mkdir: cannot create directory ‘data’: File exists\n",
            "mv: cannot stat 'train': No such file or directory\n",
            "mv: cannot stat 'test': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# データを解凍する\n",
        "!unzip data.zip\n",
        "!mkdir data\n",
        "!mv train test data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZmntYPt_ITA"
      },
      "source": [
        "omnicampus 演習環境では，data_download.ipynb のマウント，zip 化，drive へのコピーを実行しないことで，\"data.zip\" を解凍した形で配置されます．したがって，data ディレクトリが存在するディレクトリをカレントディレクトリとするだけで良いです．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbxoYYC9_ITB"
      },
      "outputs": [],
      "source": [
        "# omnicampus 実行用\n",
        "# 以下の例では/workspace/Segmentation/split_data_scripts/omnicampus に data ディレクトリがあると想定\n",
        "# %cd /workspace/Segmentation/split_data_scripts_omnicampus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKQQYWiO_ITB"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy==1.22.2 h5py scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ps9z7F3E4zk"
      },
      "source": [
        "# import library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRP3Qct-OKVa",
        "outputId": "a3e0d724-3da2-4e1d-bbad-f71ea49261f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DtvdZmhKh9t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asqo5icz-gJR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ColorJitter,\n",
        "    GaussianBlur,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        "    Normalize,\n",
        "    Lambda,\n",
        "    InterpolationMode\n",
        ")\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8HLH2R0_dPn"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38jvO_Nz_cXD"
      },
      "outputs": [],
      "source": [
        "# カラーマップ生成関数：セグメンテーションの可視化用\n",
        "def colormap(N=256, normalized=False):\n",
        "    def bitget(byteval, idx):\n",
        "        return ((byteval & (1 << idx)) != 0)\n",
        "\n",
        "    dtype = 'float32' if normalized else 'uint8'\n",
        "    cmap = np.zeros((N, 3), dtype=dtype)\n",
        "    for i in range(N):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r = r | (bitget(c, 0) << 7-j)\n",
        "            g = g | (bitget(c, 1) << 7-j)\n",
        "            b = b | (bitget(c, 2) << 7-j)\n",
        "            c = c >> 3\n",
        "\n",
        "        cmap[i] = np.array([r, g, b])\n",
        "\n",
        "    cmap = cmap/255 if normalized else cmap\n",
        "    return cmap\n",
        "\n",
        "# NYUv2データセット：RGB画像、セgmentation、深度、法線マップを提供するデータセット\n",
        "class NYUv2(VisionDataset):\n",
        "    \"\"\"NYUv2 dataset\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        split (string, optional): 'train' for training set, and 'test' for test set. Default: 'train'.\n",
        "        target_type (string, optional): Type of target to use, ``semantic``, ``depth``.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version.\n",
        "        target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n",
        "        depth_transform (callable, optional): A function/transform that takes in a PIL depth image and returns a transformed version.\n",
        "    \"\"\"\n",
        "    cmap = colormap()\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 split='train',\n",
        "                 include_depth=False,\n",
        "                 transform=None,\n",
        "                 target_transform=None,\n",
        "                 depth_transform=None\n",
        "                 ):\n",
        "        super(NYUv2, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "        # データセットの基本設定\n",
        "        assert(split in ('train', 'test'))\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.include_depth = include_depth\n",
        "        self.train_idx = np.array([255, ] + list(range(13)))  # 13クラス分類用\n",
        "        self.depth_transform = depth_transform\n",
        "\n",
        "        # 画像ファイルのパスリストを作成\n",
        "        img_names = os.listdir(os.path.join(self.root, self.split, 'image'))\n",
        "        img_names.sort()\n",
        "        images_dir = os.path.join(self.root, self.split, 'image')\n",
        "        self.images = [os.path.join(images_dir, name) for name in img_names]\n",
        "\n",
        "        label_dir = os.path.join(self.root, self.split, 'label')\n",
        "        if (self.split == 'train'):\n",
        "          self.labels = [os.path.join(label_dir, name) for name in img_names]\n",
        "          self.targets = self.labels\n",
        "\n",
        "        depth_dir = os.path.join(self.root, self.split, 'depth')\n",
        "        self.depths = [os.path.join(depth_dir, name) for name in img_names]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx]).convert('RGB') # Ensure RGB\n",
        "        depth = Image.open(self.depths[idx]).convert('L') # Ensure grayscale\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.depth_transform is not None:\n",
        "            depth = self.depth_transform(depth)\n",
        "\n",
        "        if self.include_depth:\n",
        "            # Stack RGB and Depth channels\n",
        "            image = torch.cat([image, depth], dim=0)\n",
        "\n",
        "        if self.split=='test':\n",
        "            return image\n",
        "\n",
        "        if self.split == 'train' and self.target_transform is not None:\n",
        "            target = Image.open(self.targets[idx])\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgWaxx12628b"
      },
      "source": [
        "# Model Section\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else cpu)"
      ],
      "metadata": {
        "id": "-GanIYHwLI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0,5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "num_classes = 13\n",
        "\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "fce66db6c8ca47ec88ec8a2c1aabb1b3",
            "7ec74494ebb9470ca00d03a7cdc2a6c2",
            "6127b014d9ed46e49d6c4d79a8b16ecf",
            "da67c866bbbb46838ed2af9ef1bd80c2",
            "7a652dda54e74d36880b7cf47d53071b",
            "4d9a2fb673964151803b298a972498e4",
            "bf566ef2ff0b4f15b16305053f7f43e4",
            "ed3c1278fc2c41c6a71ad0e7e393a4a6",
            "a5794821f2614416b690ae295e35db2e",
            "a9f3a90ff7b544c4a71231a5e9f9fdfd",
            "c83639776b314b7f875e39f0cc028195"
          ]
        },
        "id": "sNhmjv5_Ldkf",
        "outputId": "5d082f96-bc62-4d3c-e8d4-282db3488934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fce66db6c8ca47ec88ec8a2c1aabb1b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q03fys06aKJ"
      },
      "outputs": [],
      "source": [
        "# 2つの畳み込み層とバッチ正規化、ReLUを含むブロック\n",
        "# UNetの各層で使用される基本的な畳み込みブロック\n",
        "# class DoubleConv(nn.Module):\n",
        "#    def __init__(self, in_channels, out_channels):\n",
        "#         super().__init__()\n",
        "#         self.double_conv = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.double_conv(x)\n",
        "\n",
        "# UNetモデル：エンコーダ・デコーダ構造のセグメンテーションモデル\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels, num_classes):\n",
        "#         super().__init__()\n",
        "#         # エンコーダ部分：特徴量の抽出と空間サイズの縮小\n",
        "#         self.enc1 = DoubleConv(in_channels, 64)\n",
        "#         self.enc2 = DoubleConv(64, 128)\n",
        "#         self.enc3 = DoubleConv(128, 256)\n",
        "#         self.enc4 = DoubleConv(256, 512)\n",
        "#         self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "#         # デコーダ部分：特徴量の統合と空間サイズの復元\n",
        "#         self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "#         self.dec3 = DoubleConv(512 + 256, 256)\n",
        "#         self.dec2 = DoubleConv(256 + 128, 128)\n",
        "#         self.dec1 = DoubleConv(128 + 64, 64)\n",
        "\n",
        "#         # 最終層：クラス数に応じた出力チャネルに変換\n",
        "#         self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # エンコーダパス：特徴抽出とダウンサンプリング\n",
        "#         e1 = self.enc1(x)\n",
        "#         e2 = self.enc2(self.pool(e1))\n",
        "#         e3 = self.enc3(self.pool(e2))\n",
        "#         e4 = self.enc4(self.pool(e3))\n",
        "\n",
        "#         # デコーダパス：特徴統合とアップサンプリング（スキップ接続を使用）\n",
        "#         d3 = self.dec3(torch.cat([self.up(e4), e3], dim=1))\n",
        "#         d2 = self.dec2(torch.cat([self.up(d3), e2], dim=1))\n",
        "#         d1 = self.dec1(torch.cat([self.up(d2), e1], dim=1))\n",
        "\n",
        "#         return self.final(d1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzcggBG-8MnN"
      },
      "source": [
        "# Train and Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9YiGhXjB9IK"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # データセットパス\n",
        "    dataset_root: str = \"data\"\n",
        "\n",
        "    # データ関連\n",
        "    batch_size: int = 32\n",
        "    num_workers: int = 4\n",
        "\n",
        "    # モデル関連\n",
        "    in_channels: int = 3\n",
        "    num_classes: int = 13  # NYUv2データセットの場合\n",
        "\n",
        "    # 学習関連\n",
        "    epochs: int = 100\n",
        "    learning_rate: float = 0.001\n",
        "    weight_decay: float = 1e-4\n",
        "\n",
        "    # データ分割関連\n",
        "    train_val_split: float = 0.8  # 訓練データの割合\n",
        "\n",
        "    # デバイス設定\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # チェックポイント関連\n",
        "    checkpoint_dir: str = \"checkpoints\"\n",
        "    save_interval: int = 5  # エポックごとのモデル保存間隔\n",
        "\n",
        "    # データ拡張・前処理関連\n",
        "    image_size: tuple = (256, 256)\n",
        "    normalize_mean: tuple = (0.485, 0.456, 0.406)  # ImageNetの標準化パラメータ\n",
        "    normalize_std: tuple = (0.229, 0.224, 0.225)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        import os\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0OFbNO2DsCR"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    シードを固定する．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        乱数生成に用いるシード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF4K05BI7wDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c5d328f-dedb-4fcf-c024-210ddbb0521d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "on epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-22-3898750863.py:116: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-16-3380550403.py\", line 69, in __getitem__\n    image = self.transform(image)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\", line 350, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-3898750863.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"on epoch: {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-16-3380550403.py\", line 69, in __getitem__\n    image = self.transform(image)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\", line 350, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "# 設定の初期化\n",
        "config = TrainingConfig(\n",
        "    dataset_root='/content/data',\n",
        "    batch_size=16,\n",
        "    num_workers=4,\n",
        "    learning_rate=1e-4,\n",
        "    epochs=100,\n",
        "    image_size=(320, 240),\n",
        "    in_channels=4  # RGB(3チャネル) + Depth(1チャネル)\n",
        ")\n",
        "\n",
        "'''\n",
        "データセットのディレクトリ構造：\n",
        "    data/NYUv2/\n",
        "    ├─train/\n",
        "    │  ├─image/      # RGB画像（入力）\n",
        "    │  │    000000.png\n",
        "    │  │    ...\n",
        "    |  ├─depth/      # 深度画像（入力）\n",
        "    |  │    000000.png\n",
        "    |  │    ...\n",
        "    │  └─label/      # 13クラスセグメンテーション（教師ラベル）\n",
        "    │       000000.png\n",
        "    │       ...\n",
        "    └─test/\n",
        "       ├─image/      # RGB画像（入力）\n",
        "       │    000000.png\n",
        "       │    ...\n",
        "       ├─depth/      # 深度画像（入力）\n",
        "       │    000000.png\n",
        "       │    ...\n",
        "'''\n",
        "\n",
        "\n",
        "# ------------------\n",
        "#    Dataloader\n",
        "# ------------------\n",
        "\n",
        "# データ前処理の定義\n",
        "# RGB画像のTransform：リサイズとテンソル変換、正規化\n",
        "transform = Compose([\n",
        "    Resize(config.image_size, interpolation=InterpolationMode.BILINEAR),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=config.normalize_mean, std=config.normalize_std) # Use ImageNet stats for RGB\n",
        "])\n",
        "\n",
        "# Depth画像のTransform：リサイズとテンソル変換、正規化 (単一チャンネル)\n",
        "depth_transform = Compose([\n",
        "    Resize(config.image_size, interpolation=InterpolationMode.BILINEAR),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5,), std=(0.5,)) # Normalize depth to [-1, 1]\n",
        "])\n",
        "\n",
        "\n",
        "# セグメンテーションラベルのTransform：リサイズとテンソル変換\n",
        "target_transform = Compose([\n",
        "    Resize(config.image_size, interpolation=InterpolationMode.NEAREST),\n",
        "    Lambda(lambda lbl: torch.from_numpy(np.array(lbl)).long())\n",
        "])\n",
        "\n",
        "# データセットの準備\n",
        "# RGBデータセットとセグメンテーションラベルの読み込み\n",
        "train_dataset = NYUv2(\n",
        "    root=config.dataset_root,\n",
        "    split='train',\n",
        "    include_depth=True,\n",
        "    transform=transform,\n",
        "    target_transform=target_transform,\n",
        "    depth_transform=depth_transform\n",
        ")\n",
        "\n",
        "# テストデータセット\n",
        "test_dataset = NYUv2(\n",
        "    root=config.dataset_root,\n",
        "    split='test',\n",
        "    include_depth=True,\n",
        "    transform=transform,\n",
        "    depth_transform=depth_transform\n",
        ")\n",
        "\n",
        "\n",
        "'''\n",
        "    train data:\n",
        "        Type of batch: tuple\n",
        "        Index 0 (入力データ):\n",
        "            Type: torch.Tensor\n",
        "            Shape: torch.Size([Batch, 4, N, M])\n",
        "            Details: RGBDテンソル (RGB + Depth)\n",
        "                    - チャネル0-2: RGB画像 (正規化済み)\n",
        "                    - チャネル3: 深度画像 (正規化済み)\n",
        "        Index 1 (教師ラベル):\n",
        "            Type: torch.Tensor\n",
        "            Shape: torch.Size([Batch, N, M])\n",
        "            Details: セグメンテーションマップ\n",
        "                    - 値域: 0-12 (13クラス)\n",
        "                    - 255: ignore index\n",
        "\n",
        "    test data:\n",
        "        Type of batch: torch.Tensor\n",
        "        Shape: torch.Size([Batch, 4, N, M])\n",
        "        Details: RGBD画像 (正規化済み)\n",
        "'''\n",
        "\n",
        "# データローダーの作成\n",
        "train_data = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "test_data = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=config.num_workers)\n",
        "\n",
        "# モデルとトレーニングの設定\n",
        "device = config.device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------\n",
        "#    Model\n",
        "# ------------------\n",
        "# model = UNet(in_channels=config.in_channels, num_classes=config.num_classes).to(device)\n",
        "\n",
        "# ------------------\n",
        "#    optimizer\n",
        "# ------------------\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "# ------------------\n",
        "#    Training\n",
        "# ------------------\n",
        "num_epochs = config.epochs\n",
        "scaler = GradScaler()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    print(f\"on epoch: {epoch+1}\")\n",
        "    for inputs, labels in train_data: # inputs will be the concatenated RGBD tensor\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n",
        "\n",
        "# モデルの保存\n",
        "current_time = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "model_path = f\"model_{current_time}.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uMIiVEdEEtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6464b4a-d81e-4b74-8b17-db52e2906f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 654/654 [00:23<00:00, 27.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission.npy\n"
          ]
        }
      ],
      "source": [
        "# ------------------\n",
        "#    Evaluation\n",
        "# ------------------\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# 予測結果の生成\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(\"Generating predictions...\")\n",
        "    for inputs in test_data: # inputs will be the concatenated RGBD tensor\n",
        "        inputs = inputs.to(device)\n",
        "        output = model(inputs)            # [Batch, num_classes, H, W]\n",
        "        pred = output.argmax(dim=1)  # [Batch, H, W]\n",
        "        predictions.append(pred.cpu())\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "\n",
        "predictions = predictions.cpu().numpy()\n",
        "np.save('submission.npy', predictions)\n",
        "print(\"Predictions saved to submission.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdlb2A8I_pUD"
      },
      "source": [
        "## 提出方法\n",
        "\n",
        "以下の3点をzip化し，Omnicampusの「最終課題 (セグメンテーション)」から提出してください．\n",
        "\n",
        "- `submission.npy`\n",
        "- `model.pt`や`model_best.pt`など，テストに使用した重み（拡張子は`.pt`のみ）\n",
        "- 本Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0vBGHo__oYw"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "\n",
        "notebook_path = \"/content/drive/MyDrive/DLBasics2025_colab/最終課題/NYUv2\"\n",
        "\n",
        "with ZipFile(\"submission.zip\",\n",
        "             mode=\"w\",\n",
        "             compression=ZIP_DEFLATED,\n",
        "             compresslevel=9) as zf:\n",
        "    zf.write(\"submission.npy\")\n",
        "    zf.write(model_path)\n",
        "    zf.write(notebook_path,\n",
        "             arcname=\"DL_Basic_2025_Competition_NYUv2_baseline.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqb5TnRq_ITD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fce66db6c8ca47ec88ec8a2c1aabb1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ec74494ebb9470ca00d03a7cdc2a6c2",
              "IPY_MODEL_6127b014d9ed46e49d6c4d79a8b16ecf",
              "IPY_MODEL_da67c866bbbb46838ed2af9ef1bd80c2"
            ],
            "layout": "IPY_MODEL_7a652dda54e74d36880b7cf47d53071b"
          }
        },
        "7ec74494ebb9470ca00d03a7cdc2a6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9a2fb673964151803b298a972498e4",
            "placeholder": "​",
            "style": "IPY_MODEL_bf566ef2ff0b4f15b16305053f7f43e4",
            "value": "model.safetensors: 100%"
          }
        },
        "6127b014d9ed46e49d6c4d79a8b16ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed3c1278fc2c41c6a71ad0e7e393a4a6",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5794821f2614416b690ae295e35db2e",
            "value": 346284714
          }
        },
        "da67c866bbbb46838ed2af9ef1bd80c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f3a90ff7b544c4a71231a5e9f9fdfd",
            "placeholder": "​",
            "style": "IPY_MODEL_c83639776b314b7f875e39f0cc028195",
            "value": " 346M/346M [00:01&lt;00:00, 337MB/s]"
          }
        },
        "7a652dda54e74d36880b7cf47d53071b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9a2fb673964151803b298a972498e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf566ef2ff0b4f15b16305053f7f43e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed3c1278fc2c41c6a71ad0e7e393a4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5794821f2614416b690ae295e35db2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9f3a90ff7b544c4a71231a5e9f9fdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83639776b314b7f875e39f0cc028195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}